{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "week 3 미션 제출\n",
    "============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 넘파이 numpy import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. 본격적으로 Numpy와 친해지기 위해서 다양한 연산을 연습해볼 예정입니다. <span style=\"color: #008000\"> **무작위의 데이터를 가 진 5x3의 행렬을 가지는 numpy array와 3x2 행렬을 가지는 numpy array를 만든 후 행열곱 연산**</span>을 진행해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.random.rand(5,3)\n",
    "arr2 = np.random.rand(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = arr1@arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.80886351 0.64687835]\n",
      " [0.35089326 0.24608097]\n",
      " [1.28453036 1.00859289]\n",
      " [1.01833717 0.79018664]\n",
      " [0.96711353 0.7381028 ]] (5, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dot, dot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. 두번째로는 numpy에서 자주 사용하는 concatenate 연산에 대한 미션을 수행해보겠습니다. 이제 Numpy에서 사용하는 2개의 numpy array가 있을때, <span style=\"color: #008000\">**두 numpy array의 concatenate 연산**</span>을 구해보세요.\n",
    "\n",
    "* axis는 0, 1 두개로 연산한 결과를 출력해보세요.\n",
    "* 각 데이터가 어떤 형태로 더해지는지 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.array([[5, 7], [9, 11]], float)\n",
    "arr2 = np.array([[2, 4], [6, 8]], float)\n",
    "\n",
    "concat_1 = np.concatenate((arr1, arr2), axis=0) # axis 0\n",
    "concat_2 = np.concatenate((arr1, arr2), axis=1) # axis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  7.]\n",
      " [ 9. 11.]\n",
      " [ 2.  4.]\n",
      " [ 6.  8.]]\n",
      "[[ 5.  7.  2.  4.]\n",
      " [ 9. 11.  6.  8.]]\n"
     ]
    }
   ],
   "source": [
    "print(concat_1)\n",
    "print(concat_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. 3번부터 5번까지의 미션는 Numpy를 이용해서 정답값을 예측해보는 linear regression을 구현해 보는 미션입니다. 첫번째 단계로 데이터를 준비해보도록 하겠습니다. 아래와 같이 데이터가 주어져있을 때, <span style=\"color: #008000\">**경사하강법을 위한 데이터를 분리**</span>해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array([[1., 2., 3., 4., 5., 6.],\n",
    "               [10., 20., 30., 40., 50., 60.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3 첫번째 방법 - slicing 이용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5. 6.] (6,)\n",
      "[10. 20. 30. 40. 50. 60.] (6,)\n"
     ]
    }
   ],
   "source": [
    "x_train = xy[0]\n",
    "y_train = xy[1]\n",
    "\n",
    "print(x_train, x_train.shape)\n",
    "print(y_train, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3 두번째 방법 - unpacking 이용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5. 6.] (6,)\n",
      "[10. 20. 30. 40. 50. 60.] (6,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = xy\n",
    "\n",
    "print(x_train, x_train.shape)\n",
    "print(y_train, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. <span style=\"color: #008000\">**경사 하강법 구현을 위해서 위에서 분리한 x_train 데이터와 계산될 weight, bias 값을 정의해보세요.**</span> 여기서 구현한 weight와 bias 는 linear regression 계산을 진행할시 직선의 기울기와 y 절편의 값이 됩니다.\n",
    "* numpy내의 random 함수를 이용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta_gd, bias 초기화\n",
    "beta_gd = np.random.rand(1) # weight (기울기)\n",
    "bias = np.random.rand(1) # y절편"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61624071] [0.00929793]\n"
     ]
    }
   ],
   "source": [
    "print(beta_gd, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5. 이제 최종적으로 <span style=\"color: #008000\">**linear regression을 경사하강법으로 학습하는 코드를 구현**</span>해봅시다. 경사하강법 구현을 위한 학습 Loop를 구현해보세요. 최종적으로 1000회 반복했을 시에 결과를 출력하세요.\n",
    "* 단, Error는 차이의 제곱을 이용해서 계산해주세요.\n",
    "* Gradient 값은 우리가 예측하는 각 변수에 대한 미분값입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (         0/1000) error: 1334.889238 beta_gd:   9.153509 bias:   1.979330\n",
      "Epoch (       100/1000) error:   0.092730 beta_gd:   9.839627 bias:   0.686589\n",
      "Epoch (       200/1000) error:   0.010257 beta_gd:   9.946663 bias:   0.228347\n",
      "Epoch (       300/1000) error:   0.001135 beta_gd:   9.982261 bias:   0.075944\n",
      "Epoch (       400/1000) error:   0.000125 beta_gd:   9.994100 bias:   0.025258\n",
      "Epoch (       500/1000) error:   0.000014 beta_gd:   9.998038 bias:   0.008400\n",
      "Epoch (       600/1000) error:   0.000002 beta_gd:   9.999347 bias:   0.002794\n",
      "Epoch (       700/1000) error:   0.000000 beta_gd:   9.999783 bias:   0.000929\n",
      "Epoch (       800/1000) error:   0.000000 beta_gd:   9.999928 bias:   0.000309\n",
      "Epoch (       900/1000) error:   0.000000 beta_gd:   9.999976 bias:   0.000103\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "X = np.array([np.append(x, [1]) for x in x_train]) # 상수항 추가\n",
    "beta = np.array([*beta_gd, *bias]) # beta[0]: beta_gd, beta[1]: bias\n",
    "\n",
    "for i in range(1000):\n",
    "    dif = y_train - X @ beta # y_train - X @ beta로 실제 y값과 계산된 값의 차이를 구한다.\n",
    "    grad =  - np.transpose(X) @ dif # X의 전치행렬을 dif에 곱해 gradient를 구한다.\n",
    "    beta = beta - learning_rate * grad # beta 값을 learning rate에 비례하여 업데이트한다.\n",
    "    error = (dif**2).mean() # error는 차이의 제곱을 이용해 계산한다.\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        # 출력 결과 Q3의 x, y 값에 맞게 y = 10 * x + 0 이 되도록 beta, 즉, 기울기와 y절편이 조정된다.\n",
    "        print('Epoch ({:10d}/1000) error: {:10f} beta_gd: {:10f} bias: {:10f}'.format(i, error, beta[0].item(), beta[1].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "653d79336d89b7ad33e2a2f4c80c1cdae17151a2236a2d1b85b40cc72bd8e1b5"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
